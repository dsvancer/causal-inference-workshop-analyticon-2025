{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e5d8c8",
   "metadata": {},
   "source": [
    "# Machine Learning with `scikit-learn`\n",
    "\n",
    "Modern causal inference is a blend of causal theory, graphical models, mathematical statistics and machine learning. To use causal inference techniques, one needs to have a solid understanding of these fields to ensure accurate estimation of causal effects.\n",
    "\n",
    "The most popular causal ML packages in Python, including `econml`, `causallib`, `doubleml`, and `causalml`, all utilize `scikit-learn` as their backend ML library. \n",
    "\n",
    "This section is a quick introduction to fitting models with `scitkit-learn` so that we can understand how `doubleml` is used in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92763a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\") \n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['font.family'] = 'monospace'\n",
    "\n",
    "### Machine Learning\n",
    "## Cross validation and data resampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Model Evaluation\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_score, \n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc48df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load observational dataset\n",
    "observational_df = pd.read_pickle('../data/observational_df.pkl').drop(columns=['upsell_marketing'])\n",
    "\n",
    "# Identify columns\n",
    "customer_features = observational_df.drop(columns=['amu_signup']).columns.to_list()\n",
    "target_outcome = 'amu_signup'\n",
    "\n",
    "print('Customer Features: ', customer_features)\n",
    "\n",
    "observational_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd4ebe",
   "metadata": {},
   "source": [
    "## Basic Machine Learning Workflow\n",
    "Below is a commonly used workflow for training machine learning algorithms. The key to randomly split our dataset into a training and test set to ensure optimal performance on new datasets\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img \n",
    "  src=\"../assets/model_process.png\" \n",
    "  alt=\"Modeling Process\" \n",
    "  style=\"width:auto;height:300px;\"\n",
    "> \n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d8a2f",
   "metadata": {},
   "source": [
    "## Data Resampling\n",
    "Splitting our data into a training and test set is important for evaluating model performance and estimating generalization to new data\n",
    "\n",
    "- **Under-fitting**\n",
    "    - Model can't capture complex trends in the data\n",
    "    - Give away - poor performance on both training and test datasets\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img \n",
    "  src=\"../assets/under_fitting.png\" \n",
    "  alt=\"Underfitting\" \n",
    "  style=\"width:auto;height:375px;\"\n",
    "> \n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66bd8b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- **Over-fitting**\n",
    "    - Model finds trends that don't exist\n",
    "    - Give away - great performance training data and *poor performance test dataset*\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img \n",
    "  src=\"../assets/under_fitting.png\" \n",
    "  alt=\"Underfitting\" \n",
    "  style=\"width:auto;height:375px;\"\n",
    "> \n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d223df1",
   "metadata": {},
   "source": [
    "### Optimal Complexity\n",
    "\n",
    "Generally, as we go from simple models to more complex:\n",
    "- Training error continues to decrease (potentially reaching zero!)\n",
    "- Test error decreases initially, but increases when we are over-fitting\n",
    "- Goal is to find the optimal model complexity to ensure good performance on new data\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img \n",
    "  src=\"../assets/optimal_complexity.png\" \n",
    "  alt=\"Underfitting\" \n",
    "  style=\"width:auto;height:375px;\"\n",
    "> \n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f6b18f",
   "metadata": {},
   "source": [
    "### Creating Training and Test Datasets with `scikit-learn`\n",
    "The `train_test_split` function can randomly divide our original dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d5984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    observational_df, \n",
    "    train_size = 0.7, \n",
    "    stratify=observational_df[target_outcome])\n",
    "\n",
    "# Check dataset properties\n",
    "print(\n",
    "    f'Training Rows: {train_df.shape[0]:,}',\n",
    "    f'Training Signup Rate: {train_df[target_outcome].mean():.1%}',\n",
    "    f'Test Rows: {test_df.shape[0]:,}',\n",
    "    f'Test Signup Rate: {test_df[target_outcome].mean():.1%}',\n",
    "    sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a90570",
   "metadata": {},
   "source": [
    "## Modeling with `scikit-learn`\n",
    "\n",
    "### Introduction to Classification\n",
    "\n",
    "In classification, we are predicting a target outcome with categorical values. Typically the category of interest which we are predicting is labeled as the **positive class**. \n",
    "\n",
    "The positive class for our data would be 1. indicating a signup. The remaining outcome category is the negative class (0 in our data).\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img \n",
    "  src=\"../assets/classification.png\" \n",
    "  alt=\"Classification\" \n",
    "  style=\"width:auto;height:400px;\"\n",
    "> \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05de154e",
   "metadata": {},
   "source": [
    "#### Finding the Optimal Decision Boundary\n",
    "\n",
    "The goal of classification is to find an optimal decision boundary which splits the feature space into two distinct regions, one where we predict the positive class and the other the negative class. \n",
    "\n",
    "The decision boundary is a function where the estimated probability of the positive outcome is equal to 0.5. \n",
    "Feature combinations above this line have $P(Outcome = Positive class) > 0.5$ and are classified as a positive outcome.\n",
    "\n",
    "The decision boundary below is a linear boundary. Models such as **Logistic Regression** form linear decision boundaries.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img \n",
    "  src=\"../assets/classification_boundary.png\" \n",
    "  alt=\"Classification Boundary\" \n",
    "  style=\"width:auto;height:400px;\"\n",
    "> \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8787c",
   "metadata": {},
   "source": [
    "The great about `scikit-learn` is that each model is defined as its own object with `fit()` and `predict()` methods. Once a model has been specified, then the syntax for training and performance evaluation remains the same.\n",
    "\n",
    "A huge benefit of using `scikit-learn` for machine learning is its amazing [documentation](https://scikit-learn.org/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eba793",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
